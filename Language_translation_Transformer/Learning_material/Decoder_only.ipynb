{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30   # Each batch will contain 30 sentences\n",
    "max_sequence_len = 200  # max sentence len will be 200\n",
    "d_model = 512 # Dimensionality of each char in sequence i.e 200 x 512\n",
    "number_heads = 8  # Number of attention heads\n",
    "fnn_hidden = 2048 # Feedforward layer dim\n",
    "drop_prob = 0.1 # Dropout \n",
    "num_layer = 5 #number of layers  of encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametersConfig():\n",
    "    def __init__(self,**kwargs):\n",
    "        self.batch_size = 30   \n",
    "        self.max_sequence_len = 200  \n",
    "        self.d_model = 512 \n",
    "        self.number_heads = 8  \n",
    "        self.fnn_hidden = 2048 \n",
    "        self.drop_prob = 0.1 \n",
    "        self.num_layer = 5\n",
    "\n",
    "        for key,val in kwargs.items():\n",
    "            setattr(self,key,val)\n",
    "\n",
    "    def display(self):\n",
    "        print(\"parameters are: \")\n",
    "        for key,val in vars(self).items():\n",
    "            print(f\"{key} = {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters are: \n",
      "batch_size = 30\n",
      "max_sequence_len = 200\n",
      "d_model = 512\n",
      "number_heads = 8\n",
      "fnn_hidden = 2048\n",
      "drop_prob = 0.1\n",
      "num_layer = 5\n"
     ]
    }
   ],
   "source": [
    "config = ParametersConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = torch.rand(batch_size,max_sequence_len,d_model)\n",
    "sen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalardot(q,k,v,mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scalar = (q@k.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scalar+=mask\n",
    "    soft = F.softmax(scalar,dim=-1)\n",
    "    out = soft@v\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.number_heads == 0\n",
    "        self.in_linear = nn.Linear(config.d_model,config.d_model*3)\n",
    "        self.out_linear = nn.Linear(config.d_model,config.d_model)\n",
    "        self.d_model = config.d_model\n",
    "        self.number_heads = config.number_heads\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.size()\n",
    "        q,k,v = self.in_linear(x).split(C,dim=-1)\n",
    "        q = q.view(B,T,self.number_heads,C//self.number_heads).transpose(1,2)\n",
    "        k = k.view(B,T,self.number_heads,C//self.number_heads).transpose(1,2)\n",
    "        v = v.view(B,T,self.number_heads,C//self.number_heads).transpose(1,2)\n",
    "        attention = scalardot(q,k,v,mask=None)\n",
    "        x = attention.transpose(1,2).reshape(B,T,C)\n",
    "        out = self.out_linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi = MultHeadAttention(config)\n",
    "# new = multi(sen)\n",
    "# new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask = torch.full([10, 10] , float('-inf'))\n",
    "# mask = torch.triu(mask, diagonal=1)\n",
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters are: \n",
      "batch_size = 30\n",
      "max_sequence_len = 200\n",
      "d_model = 512\n",
      "number_heads = 8\n",
      "fnn_hidden = 2048\n",
      "drop_prob = 0.1\n",
      "num_layer = 5\n"
     ]
    }
   ],
   "source": [
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.in_linear = nn.Linear(config.d_model,config.fnn_hidden)\n",
    "        self.out_linear = nn.Linear(config.fnn_hidden,config.d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(config.drop_prob)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.in_linear(x)\n",
    "        print(f\"input after first linear layer: {x.shape}\")\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.out_linear(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input after first linear layer: torch.Size([30, 200, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_layer = PositionwiseFeedForward(config)\n",
    "fnn_layer(sen).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,config,elp=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameter_shape = config.d_model\n",
    "        self.elp = elp\n",
    "        self.gamma = nn.Parameter(torch.ones(self.parameter_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.parameter_shape))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        var = ((x-mean)**2).mean(dim=-1,keepdim=True)\n",
    "        std = (var+self.elp).sqrt()\n",
    "        x = (x-var)/std\n",
    "        out = x*self.gamma + self.beta\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer_norm = LayerNormalization(config)\n",
    "# layer_norm(sen).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.vk_linear = nn.Linear(config.d_model,config.d_model*2)\n",
    "        self.q_linear = nn.Linear(config.d_model,config.d_model)\n",
    "        self.out_linear = nn.Linear(config.d_model,config.d_model)\n",
    "        self.num_head = config.number_heads\n",
    "        self.d_model = config.d_model\n",
    "\n",
    "    def forward(self,x,y,mask): #--> y = query , x = value,key\n",
    "        B,T,C = x.size()\n",
    "        k,v = self.vk_linear(x).split(self.d_model,dim=-1)\n",
    "        q = self.q_linear(y)\n",
    "        q = q.view(B,T,self.num_head,C//self.num_head).transpose(1,2)\n",
    "        k = k.view(B,T,self.num_head,C//self.num_head).transpose(1,2)\n",
    "        v = v.view(B,T,self.num_head,C//self.num_head).transpose(1,2)\n",
    "        attention = scalardot(q,k,v,mask)\n",
    "        value = attention.transpose(1,2).reshape(B,T,C)\n",
    "        out = self.out_linear(value)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full([config.max_sequence_len,config.max_sequence_len],float(-1e9))\n",
    "mask = torch.triu(mask,diagonal=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 200, 512]),\n",
       " tensor([-3.4942e-01, -4.0296e-01, -9.0418e-02, -2.6804e-02,  3.6759e-01,\n",
       "         -2.4123e-01, -2.8574e-01,  1.0380e-01,  5.5429e-02, -3.3417e-01,\n",
       "         -3.6635e-02, -3.2199e-01,  1.4780e-01, -2.1409e-02,  2.3340e-01,\n",
       "          1.3015e-02,  3.6251e-02,  1.5289e-02, -4.9355e-02, -1.3778e-01,\n",
       "         -1.0361e-01, -2.0159e-01,  1.9469e-01,  1.4734e-01, -3.9139e-02,\n",
       "         -8.1272e-02,  9.2873e-02,  3.1316e-02, -1.4936e-01, -3.3456e-02,\n",
       "          3.2958e-02, -9.7513e-02,  6.1522e-02,  2.8603e-01, -8.2449e-02,\n",
       "         -5.1152e-01, -7.9984e-02,  1.6377e-02, -4.1719e-02, -2.4613e-01,\n",
       "          2.9034e-02,  1.6266e-01, -1.2981e-01, -2.4003e-02, -1.1398e-01,\n",
       "          1.6889e-02, -2.0202e-02,  1.3650e-01, -2.1261e-01,  2.5188e-02,\n",
       "          1.6057e-01,  1.4460e-01, -1.7118e-02,  2.0981e-01,  8.5745e-02,\n",
       "          4.6990e-02,  2.1997e-01,  8.2009e-03,  1.9615e-01, -1.1128e-01,\n",
       "          3.1003e-01, -2.8893e-01, -8.6054e-02, -2.5118e-01,  8.7434e-03,\n",
       "         -1.1926e-01,  3.5923e-02, -3.8067e-02,  1.8249e-01, -1.0671e-01,\n",
       "         -2.9196e-01,  7.8895e-02,  7.1673e-02,  1.7913e-01, -4.2819e-02,\n",
       "          3.1217e-01, -1.3359e-01,  2.9369e-02,  2.1081e-01,  1.6882e-01,\n",
       "         -2.0172e-01,  7.0429e-02, -1.4605e-01,  7.2438e-02,  2.9180e-01,\n",
       "         -2.4504e-01, -1.6884e-01, -4.7013e-01, -2.3985e-01, -8.6381e-02,\n",
       "         -3.4770e-01,  2.5620e-01,  1.9252e-01, -4.6744e-02,  1.1530e-01,\n",
       "          7.5621e-02,  3.4219e-01,  9.1860e-03, -5.2291e-02, -4.7295e-01,\n",
       "         -1.9721e-01,  3.9255e-02, -1.0494e-01, -1.3181e-01,  2.7718e-01,\n",
       "         -3.8742e-01,  1.4343e-02,  2.1617e-01,  1.5201e-01,  6.3904e-02,\n",
       "          2.1491e-02, -1.1588e-01,  2.3186e-01, -2.3981e-01, -3.6173e-03,\n",
       "         -9.1740e-02, -2.1095e-01, -3.9694e-02,  2.3661e-01, -1.0444e-01,\n",
       "         -2.7747e-02,  1.8166e-01, -1.2267e-01, -1.9741e-01,  4.1283e-01,\n",
       "         -2.8101e-01, -1.5171e-01, -1.4669e-01,  1.5710e-01, -1.4507e-01,\n",
       "         -1.6812e-01, -5.2490e-02,  1.8542e-01,  5.3520e-02,  9.4165e-02,\n",
       "          5.0100e-02, -9.2420e-02,  1.1733e-01, -2.7230e-01,  1.7032e-01,\n",
       "         -2.9050e-01, -1.6283e-01, -5.5362e-03, -2.3735e-01, -4.2303e-01,\n",
       "         -4.1338e-01,  6.3674e-02, -3.2013e-02,  4.5934e-02,  9.7126e-03,\n",
       "          3.4749e-02,  2.1532e-01, -7.4955e-02, -9.0061e-02,  4.5249e-03,\n",
       "          2.1093e-01,  3.4991e-02,  1.0754e-01, -6.1742e-02,  1.5643e-01,\n",
       "          3.1070e-02, -2.9870e-01, -2.5325e-01,  1.3575e-01, -5.9781e-02,\n",
       "         -1.4063e-02, -2.6327e-01,  2.8728e-01, -2.8521e-01,  1.3679e-01,\n",
       "          5.1384e-01, -1.4657e-01, -2.1109e-01, -2.9132e-05,  2.2826e-02,\n",
       "         -1.7630e-02, -2.5490e-01, -4.6855e-01,  1.1165e-01, -1.2243e-01,\n",
       "         -1.1629e-01,  1.6031e-01, -9.1218e-02, -1.4167e-01,  2.5584e-01,\n",
       "          1.3047e-02,  1.9424e-01, -1.7450e-01,  1.5980e-01,  4.2320e-02,\n",
       "         -3.7510e-01, -1.9953e-02,  2.3932e-01, -6.2151e-02, -8.8510e-02,\n",
       "          6.8481e-02, -1.5885e-01,  3.0953e-01,  1.2051e-03,  1.9931e-02,\n",
       "          2.1538e-01, -1.9931e-02,  4.9975e-02,  2.3437e-01, -1.2266e-01,\n",
       "         -1.1803e-01, -1.8463e-01, -2.9656e-02,  3.4629e-01,  1.9905e-01,\n",
       "         -2.0891e-01, -2.8021e-01,  2.4825e-01,  8.6500e-02,  1.5460e-01,\n",
       "          3.3182e-01,  9.3298e-02, -2.3756e-02, -2.6586e-02,  2.6268e-01,\n",
       "         -8.0574e-02, -2.3928e-01,  3.4132e-01, -7.0896e-02, -1.1663e-02,\n",
       "         -1.6154e-01, -6.6632e-02, -1.3054e-01,  1.9764e-01, -3.4454e-01,\n",
       "         -1.2654e-01,  9.4412e-03,  1.2104e-01,  1.8429e-01,  5.6518e-02,\n",
       "         -9.8282e-02, -2.1260e-01, -9.7343e-02,  1.2314e-01, -1.0025e-01,\n",
       "          2.9475e-01, -1.9683e-02,  1.1118e-01, -7.7590e-02,  2.6426e-01,\n",
       "         -1.4711e-02,  1.7483e-01,  2.3846e-01,  2.9719e-01,  1.4751e-01,\n",
       "         -1.1423e-01, -1.6287e-01, -1.5930e-01, -1.7538e-01, -3.2800e-01,\n",
       "         -1.8882e-01,  7.9362e-02, -1.7880e-01, -3.0559e-02,  1.6738e-01,\n",
       "          2.0008e-01, -3.6620e-02, -1.5604e-01,  7.9024e-02, -6.7152e-02,\n",
       "          1.1465e-01,  2.7253e-01, -8.1781e-02, -7.7949e-02,  8.8923e-02,\n",
       "         -7.5837e-02, -1.7341e-01,  1.1518e-02,  4.4512e-02, -1.3401e-01,\n",
       "          3.6418e-01, -1.4465e-01, -2.4511e-02,  9.6436e-02, -5.1021e-02,\n",
       "         -3.8178e-01, -8.5851e-03,  1.2529e-01,  1.9330e-01, -8.0408e-02,\n",
       "          3.1975e-01,  2.1617e-02, -1.8134e-01,  1.0602e-01, -2.0959e-01,\n",
       "          2.2012e-01,  1.3769e-01, -2.5171e-01,  1.2455e-01, -1.0184e-01,\n",
       "          5.7384e-01,  6.4895e-01, -6.4836e-02,  2.2145e-01,  6.2684e-02,\n",
       "          1.7615e-01, -1.0670e-01, -2.5787e-01, -6.2672e-02, -2.6116e-01,\n",
       "         -8.0317e-02,  1.4008e-01,  1.9248e-01, -9.1308e-02, -2.3046e-01,\n",
       "         -2.4537e-01, -2.7535e-01, -2.7640e-02, -9.7217e-02, -1.7201e-02,\n",
       "         -1.6657e-01, -4.8179e-02,  3.6634e-02,  8.8719e-02,  2.5752e-01,\n",
       "          2.6027e-01, -4.9289e-02, -1.5353e-01, -2.9824e-02, -3.4220e-01,\n",
       "         -7.6182e-03,  1.3058e-01,  1.4356e-01, -5.1327e-02, -6.7532e-02,\n",
       "         -1.7931e-02,  1.3694e-01,  1.1938e-01, -1.4190e-01,  2.6463e-01,\n",
       "          4.9137e-02, -2.6957e-01,  3.8014e-03, -6.5289e-02, -1.5484e-01,\n",
       "         -4.0813e-01, -2.4485e-02, -3.3693e-02, -1.4643e-01,  4.8311e-01,\n",
       "          2.4729e-01,  1.8005e-02, -3.9987e-01,  3.2410e-01, -4.0074e-01,\n",
       "          4.5018e-02,  1.4445e-02,  3.6733e-01,  1.4209e-01,  2.1293e-01,\n",
       "         -2.9763e-01,  2.8115e-01,  6.8063e-02,  1.8152e-01,  7.4231e-02,\n",
       "          3.1862e-01, -6.4559e-02, -5.0784e-02, -1.1820e-01, -9.5555e-02,\n",
       "         -2.5696e-01,  9.7366e-02,  1.6207e-01,  8.0184e-02,  4.4527e-01,\n",
       "          2.2909e-01,  3.1016e-01, -1.5351e-01, -4.8786e-01, -4.1494e-02,\n",
       "          2.8175e-01,  3.1135e-01, -3.4327e-01, -1.2443e-01, -4.5046e-01,\n",
       "         -9.0243e-02,  2.1280e-01,  2.9306e-01, -1.5108e-01,  1.6524e-02,\n",
       "          1.6306e-01,  1.8272e-01, -2.1907e-01,  1.0639e-01,  6.3391e-02,\n",
       "          1.9148e-01, -1.5635e-01,  1.4744e-01, -1.5652e-01, -1.6302e-01,\n",
       "         -1.4727e-01, -5.2964e-01, -2.0918e-02, -1.6205e-01, -3.6598e-02,\n",
       "         -3.0080e-01, -5.6508e-02,  1.4046e-01,  3.1890e-02, -4.2844e-02,\n",
       "          2.4336e-01, -7.2881e-02,  1.0771e-01, -4.1089e-01,  1.5232e-01,\n",
       "          3.6514e-01, -2.3079e-01,  9.7975e-02,  1.4307e-01, -2.3222e-01,\n",
       "          3.1068e-02, -1.2771e-01, -2.7636e-01,  1.8655e-01,  8.2828e-02,\n",
       "         -1.4619e-01, -2.6366e-01, -9.5729e-02,  3.0742e-01,  1.8904e-01,\n",
       "          1.3646e-01,  5.5336e-03,  9.7213e-02, -1.7568e-01, -6.4889e-02,\n",
       "         -2.1668e-01, -1.5559e-01,  2.2143e-01,  1.0868e-01,  2.1077e-01,\n",
       "         -1.5229e-01, -1.7953e-01, -3.4298e-03,  1.4410e-01, -1.8904e-01,\n",
       "          2.0796e-01,  1.7124e-01,  3.0697e-01,  1.0659e-01,  4.7526e-03,\n",
       "         -5.9826e-02, -4.9963e-02,  3.4584e-01, -1.9091e-01, -2.6530e-01,\n",
       "          6.4767e-02, -1.5164e-01, -4.8278e-01, -1.9826e-01,  1.5108e-01,\n",
       "          4.3384e-01,  4.0182e-02, -3.8377e-01, -3.2390e-02,  6.6319e-02,\n",
       "          4.4593e-01, -5.8748e-02, -3.2371e-01, -3.9483e-01, -2.5327e-02,\n",
       "         -3.3953e-01, -5.4559e-02,  5.5679e-02, -1.5335e-01,  3.3178e-01,\n",
       "          1.7889e-01, -4.0651e-01, -8.2632e-03,  1.3519e-01, -2.1263e-03,\n",
       "          5.0232e-02, -5.5990e-02, -2.2751e-01, -1.4503e-01, -6.4520e-02,\n",
       "          1.8361e-01,  3.0878e-01, -2.9087e-01, -5.5621e-02,  1.2815e-01,\n",
       "          5.1917e-01,  3.8474e-01, -1.6161e-01, -1.6380e-01,  9.3890e-02,\n",
       "         -2.8305e-02,  1.6263e-01,  4.9429e-02, -2.3753e-01,  8.7216e-02,\n",
       "          1.6764e-01, -1.4881e-01, -1.6639e-01, -1.6772e-01, -2.4641e-01,\n",
       "         -3.7362e-02, -5.4649e-01,  1.0952e-02,  4.4833e-01, -3.3876e-01,\n",
       "          2.6279e-01, -2.8612e-01, -9.2296e-02, -1.0977e-01, -3.0845e-02,\n",
       "          2.2783e-01,  1.1453e-01], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_att = MultiHeadCrossAttention(config)\n",
    "new = cross_att(sen,sen,mask)\n",
    "new.shape,new[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDecoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.norm1 = LayerNormalization(config)\n",
    "        self.drop1 = nn.Dropout(config.drop_prob)\n",
    "        self.cross_attention = MultiHeadCrossAttention(config)\n",
    "        self.norm2 = LayerNormalization(config)\n",
    "        self.drop2 = nn.Dropout(config.drop_prob)\n",
    "        self.fnn = PositionwiseFeedForward(config)\n",
    "        self.norm3 = LayerNormalization(config)\n",
    "        self.drop3 = nn.Dropout(config.drop_prob)\n",
    "\n",
    "    def forward(self,x,y,mask):\n",
    "        y_residual = y\n",
    "        print(\"-----------------SELF ATTENTION HEAD----------------\")\n",
    "        y = self.attention(y)\n",
    "        print(\"-----------------DROPOUT 1----------------\")\n",
    "        y = self.drop1(y)\n",
    "        print(\"-----------------ADD & NORMALIZE 1----------------\")\n",
    "        y = self.norm1(y+y_residual)\n",
    "\n",
    "        y_residual = y\n",
    "        print(\"-----------------CROSS ATTENTION HEAD----------------\")\n",
    "        y = self.cross_attention(x,y,mask)\n",
    "        print(\"-----------------DROPOUT 2----------------\")\n",
    "        y = self.drop2(y)\n",
    "        print(\"-----------------ADD & NORMALIZE 2----------------\")\n",
    "        y = self.norm2(y+y_residual)\n",
    "\n",
    "        y_residual = y\n",
    "        print(\"-----------------FEED FORWARD NETWORK----------------\")\n",
    "        y = self.fnn(y)\n",
    "        print(\"-----------------DROPOUT 3----------------\")\n",
    "        y = self.drop3(y)\n",
    "        print(\"-----------------ADD & NORMALIZE 3----------------\")\n",
    "        y = self.norm3(y+y_residual)\n",
    "        print(y.shape)\n",
    "\n",
    "        return y\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ly_de = LayerDecoder(config)\n",
    "# out = ly_de(sen,sen,mask)\n",
    "# out.shape,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderSequence(nn.Sequential):\n",
    "    def forward(self,*inputs):\n",
    "        x,y,mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x,y,mask)\n",
    "        return y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters are: \n",
      "batch_size = 30\n",
      "max_sequence_len = 200\n",
      "d_model = 512\n",
      "number_heads = 8\n",
      "fnn_hidden = 2048\n",
      "drop_prob = 0.1\n",
      "num_layer = 5\n"
     ]
    }
   ],
   "source": [
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.decoder = DecoderSequence(*[LayerDecoder(config) for _ in range(config.num_layer)])\n",
    "\n",
    "    def forward(self,x,y,mask):\n",
    "        y = self.decoder(x,y,mask)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------SELF ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 1----------------\n",
      "-----------------ADD & NORMALIZE 1----------------\n",
      "-----------------CROSS ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 2----------------\n",
      "-----------------ADD & NORMALIZE 2----------------\n",
      "-----------------FEED FORWARD NETWORK----------------\n",
      "input after first linear layer: torch.Size([30, 200, 2048])\n",
      "-----------------DROPOUT 3----------------\n",
      "-----------------ADD & NORMALIZE 3----------------\n",
      "torch.Size([30, 200, 512])\n",
      "-----------------SELF ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 1----------------\n",
      "-----------------ADD & NORMALIZE 1----------------\n",
      "-----------------CROSS ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 2----------------\n",
      "-----------------ADD & NORMALIZE 2----------------\n",
      "-----------------FEED FORWARD NETWORK----------------\n",
      "input after first linear layer: torch.Size([30, 200, 2048])\n",
      "-----------------DROPOUT 3----------------\n",
      "-----------------ADD & NORMALIZE 3----------------\n",
      "torch.Size([30, 200, 512])\n",
      "-----------------SELF ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 1----------------\n",
      "-----------------ADD & NORMALIZE 1----------------\n",
      "-----------------CROSS ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 2----------------\n",
      "-----------------ADD & NORMALIZE 2----------------\n",
      "-----------------FEED FORWARD NETWORK----------------\n",
      "input after first linear layer: torch.Size([30, 200, 2048])\n",
      "-----------------DROPOUT 3----------------\n",
      "-----------------ADD & NORMALIZE 3----------------\n",
      "torch.Size([30, 200, 512])\n",
      "-----------------SELF ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 1----------------\n",
      "-----------------ADD & NORMALIZE 1----------------\n",
      "-----------------CROSS ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 2----------------\n",
      "-----------------ADD & NORMALIZE 2----------------\n",
      "-----------------FEED FORWARD NETWORK----------------\n",
      "input after first linear layer: torch.Size([30, 200, 2048])\n",
      "-----------------DROPOUT 3----------------\n",
      "-----------------ADD & NORMALIZE 3----------------\n",
      "torch.Size([30, 200, 512])\n",
      "-----------------SELF ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 1----------------\n",
      "-----------------ADD & NORMALIZE 1----------------\n",
      "-----------------CROSS ATTENTION HEAD----------------\n",
      "-----------------DROPOUT 2----------------\n",
      "-----------------ADD & NORMALIZE 2----------------\n",
      "-----------------FEED FORWARD NETWORK----------------\n",
      "input after first linear layer: torch.Size([30, 200, 2048])\n",
      "-----------------DROPOUT 3----------------\n",
      "-----------------ADD & NORMALIZE 3----------------\n",
      "torch.Size([30, 200, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.2079, -4.2685, -6.2530,  ..., -3.9700, -5.2043, -5.3958],\n",
       "         [-4.1949, -4.9931, -7.0118,  ..., -3.4569, -5.0053, -5.1095],\n",
       "         [-4.4100, -4.6248, -6.3096,  ..., -3.8732, -4.8843, -5.6217],\n",
       "         ...,\n",
       "         [-4.1440, -4.7730, -6.1866,  ..., -4.0500, -4.8594, -5.5557],\n",
       "         [-4.4866, -4.3544, -6.5214,  ..., -3.7832, -4.6537, -5.7370],\n",
       "         [-4.2844, -4.2821, -6.3834,  ..., -3.5718, -4.9871, -6.0664]],\n",
       "\n",
       "        [[-4.5385, -4.9991, -6.4217,  ..., -4.0357, -4.9835, -4.9178],\n",
       "         [-4.4212, -4.5834, -6.5642,  ..., -3.8333, -4.8735, -5.5341],\n",
       "         [-4.4895, -4.5858, -6.5536,  ..., -3.7502, -4.6002, -5.1254],\n",
       "         ...,\n",
       "         [-4.3236, -4.8324, -6.6020,  ..., -3.6323, -4.5887, -5.5378],\n",
       "         [-4.2908, -4.6537, -5.9806,  ..., -4.1228, -4.9306, -5.5533],\n",
       "         [-4.1521, -4.9556, -6.3038,  ..., -4.1038, -4.3058, -5.3521]],\n",
       "\n",
       "        [[-4.0493, -4.2307, -6.6386,  ..., -4.9814, -4.9126, -5.6578],\n",
       "         [-4.5263, -4.4942, -6.8180,  ..., -3.6677, -5.1328, -5.4248],\n",
       "         [-4.5574, -4.4438, -6.4612,  ..., -4.2671, -4.9501, -6.0489],\n",
       "         ...,\n",
       "         [-3.5848, -4.5113, -5.1186,  ..., -4.1341, -5.2685, -5.6905],\n",
       "         [-4.2568, -5.3176, -6.3468,  ..., -4.2953, -4.7956, -5.4382],\n",
       "         [-4.4507, -4.7368, -6.4150,  ..., -4.5114, -4.9090, -5.0915]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.8506, -4.4620, -6.2714,  ..., -4.0118, -4.8804, -4.9623],\n",
       "         [-4.3523, -4.5569, -6.5757,  ..., -3.9788, -4.8966, -5.6633],\n",
       "         [-3.6767, -4.6905, -5.4476,  ..., -3.9659, -4.8292, -4.8277],\n",
       "         ...,\n",
       "         [-3.8158, -4.5158, -5.2474,  ..., -3.9321, -4.9041, -4.9923],\n",
       "         [-4.3383, -4.2683, -5.0257,  ..., -3.7271, -4.3824, -5.2059],\n",
       "         [-4.4553, -4.6391, -6.4799,  ..., -3.6469, -4.6749, -4.9818]],\n",
       "\n",
       "        [[-4.6912, -4.3305, -4.9072,  ..., -3.8300, -4.9814, -5.7832],\n",
       "         [-4.7157, -4.7563, -5.9578,  ..., -3.6943, -4.8424, -4.8607],\n",
       "         [-4.6543, -5.1079, -5.8655,  ..., -3.9106, -4.8047, -5.5248],\n",
       "         ...,\n",
       "         [-4.2260, -4.8101, -6.8023,  ..., -3.7048, -4.6740, -5.5037],\n",
       "         [-4.1307, -4.4912, -6.2016,  ..., -4.1354, -4.8659, -5.7518],\n",
       "         [-4.6364, -4.2415, -6.0629,  ..., -4.0379, -4.9980, -5.6231]],\n",
       "\n",
       "        [[-4.3070, -5.1108, -5.5737,  ..., -3.5931, -4.6473, -5.6593],\n",
       "         [-4.1643, -4.2148, -5.2494,  ..., -3.7515, -5.1702, -5.4049],\n",
       "         [-3.7820, -4.2264, -5.8934,  ..., -3.7387, -5.2640, -5.7823],\n",
       "         ...,\n",
       "         [-4.0524, -4.5827, -6.5990,  ..., -3.8802, -5.0168, -6.2516],\n",
       "         [-4.0221, -4.7275, -6.6708,  ..., -4.2538, -4.6532, -5.7508],\n",
       "         [-4.1210, -4.4433, -6.4255,  ..., -4.3047, -4.4661, -5.6501]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(config)\n",
    "decoder(sen,sen,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
