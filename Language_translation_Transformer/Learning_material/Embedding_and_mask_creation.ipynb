{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\bhoik\\OneDrive\\Desktop\\Transformer-Neural-Network-main\\A_my files\\hindi_10000.txt\",\"r\") as f:\n",
    "    hindi_sen = f.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n',\n",
       " 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n',\n",
       " 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\n',\n",
       " 'आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n',\n",
       " '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\n']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_sen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_hind = \"\".join(hindi_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\nऔर जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\nजैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\nआस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\nअदालत ने इस मामले में आगे की सुनवाई के लिए एक फरवरी की तारीख़ तय की\\nजहाँ पर ट्रैक को विभाजित किया जाना है, कृपया वह स्थान चुनें.\\nइसके तुरंत बाद सेना की 22 राष्ट्रीय राइफल्स (आरआर), सीआरपीएफ और पुलिस के स्पेशल ऑपरेशन ग्रुप (एसओजी) के जवानों द्वारा इलाके की घेराबंदी कर तलाशी अभियान चलाया।\\nझारखंड के मुख्यमंत्री हेमंत सोरेन (फोटोः पीटीआई)\\nसेक्टर 55/56 के एसएचओ अरविंद कुमार ने बताया कि इस मामले में आईपीसी की '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_hind[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_vocab = sorted(set(word_hind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '\\xa0', '¡', '¨', '©', 'ª', '°', '²', '¶', '»', 'Â', 'Ó', '×', 'æ', 'í', 'ï', 'ə', 'ˌ', 'ː', 'χ', 'آ', 'ا', 'ب', 'ة', 'ت', 'ر', 'ش', 'ط', 'ق', 'ك', 'ل', 'م', 'ن', 'ي', 'پ', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', '।', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '॰', 'ᐅ', '\\u200b', '\\u200c', '\\u200d', '\\u200e', '–', '—', '‘', '’', '“', '”', '•', '…', '€', '₹', '™', '⊆', '□', '▫', '◆', '\\ufeff']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hindi_vocab)\n",
    "len(hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\bhoik\\OneDrive\\Desktop\\Transformer-Neural-Network-main\\A_my files\\english_10000.txt\",\"r\") as f:\n",
    "    english_sen = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_eng = \"\".join(english_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986002"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = sorted(set(word_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(english_vocab)\n",
    "len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_vocab.insert(0,START_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '\\xa0', '¡', '¨', '©', 'ª', '°', '²', '¶', '»', 'Â', 'Ó', '×', 'æ', 'í', 'ï', 'ə', 'ˌ', 'ː', 'χ', 'آ', 'ا', 'ب', 'ة', 'ت', 'ر', 'ش', 'ط', 'ق', 'ك', 'ل', 'م', 'ن', 'ي', 'پ', 'ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', '।', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '॰', 'ᐅ', '\\u200b', '\\u200c', '\\u200d', '\\u200e', '–', '—', '‘', '’', '“', '”', '•', '…', '€', '₹', '™', '⊆', '□', '▫', '◆', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "print(hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_vocab.append(PADDING_TOKEN)\n",
    "hindi_vocab.append(END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab.insert(0,START_TOKEN)\n",
    "english_vocab.append(PADDING_TOKEN)\n",
    "english_vocab.append(END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_to_index = {k:v for v,k in enumerate(hindi_vocab)}\n",
    "index_to_hindi = {k:v for k,v in enumerate(hindi_vocab)}\n",
    "english_to_index = {k:v for v,k in enumerate(english_vocab)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoe = [english_to_index[char] for char in list('hello')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(sentence,vocab):\n",
    "    vector  = [vocab[char] for char in list(sentence)]\n",
    "    return vector\n",
    "\n",
    "def decoder(vector,vocab):\n",
    "    string = \"\".join([vocab[token] for token in vector])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 65, 76, 80, 69, 83, 72]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(\"kalpesh\",english_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kalpesh'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder([75, 65, 76, 80, 69, 83, 72],index_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140, 186, 167, 161, 188, 167, 184, 150]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(\"कॉरपोरेट\",hindi_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'कॉरपोरेट'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder([140, 186, 167, 161, 188, 167, 184, 150],index_to_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3271, 1349)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(sen) for sen in hindi_sen), max(len(sen) for sen in english_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {key: len(sen) for key,sen in enumerate(english_sen)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in dict.items():\n",
    "#     if j ==1349:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'कॉरपोरेट कार्य मंत्रालय सीसीआई ने के.रहेजा कॉर्प ग्रुप द्वारा माइंडस्पेस रीट की स्थापना से संबंधित संयोजन को मंजूरी दी भारतीय प्रतिस्पर्धा आयोग (सीसीआई) ने प्रतिस्पर्धा अधिनियम, 2002 की धारा 31(1) के तहत के.रहेजा कॉर्प ग्रुप द्वारा माइंडस्पेस रीट की स्थापना करने और रीट द्वारा लक्षित निकायों में हिस्सेदारी बेचने वाले शेयरधारकों की विशिष्ट इक्विटी अंशभागिता के अधिग्रहण से संबंधित प्रस्तावित संयोजन को मंजूरी दे दी है। इस संयोजन के पक्षकार निम्नलिखित हैं : अधिग्रहणकर्ता : माइंडस्पेस बिजनेस पार्क्स रीट (माइंडस्पेस रीट) लक्षित निकाय : के.रहेजा आईटी पार्क (हैदराबाद) लिमिटेड, इनटाइम प्रॉपर्टीज लिमिटेड, सनड्यू प्रॉपर्टीज लिमिटेड, एवाकैडो प्रॉपर्टीज एंड ट्रेडिंग (इंडिया) प्राइवेट लिमिटेड, गीगाप्लेक्स एस्टेट प्राइवेट लिमिटेड, केआरसी इन्फ्रास्ट्रक्चर एंड प्रोजेक्ट्स प्राइवेट लिमिटेड, होराइजनव्यू प्रॉपर्टीज प्राइवेट लिमिटेड और माइंडस्पेस बिजनेस पार्क्स प्राइवेट लिमिटेड माइंडस्पेस रीट की स्थापना भारतीय ट्रस्ट अधिनियम,1882 के प्रावधानों के तहत एक अंशदायी, अपरिवर्तनीय और सुव्यवस्थित ट्रस्ट के रूप में की जाती है, जिसका उद्देश्य रियल एस्टेट निवेश ट्रस्ट का संचालन करना है, ताकि रीट (रियल एस्टेट इन्वेस्टमेंट ट्रस्ट) के जरिये धन जुटाया जा सके, रीट के नियमों एवं निवेश रणनीति के अनुसार निवेश किया जा सके और उन कार्यकलापों को जारी रखा जा सके जिसकी आवश्यकता रीट के संचालन के लिए पड़ सकती है।\\n'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_sen[7750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 percent of hindi sentence has average len of: 201.0699999999997\n",
      "93 percent of english sentence has average len of: 214.0699999999997\n"
     ]
    }
   ],
   "source": [
    "per = 93\n",
    "print(f\"{per} percent of hindi sentence has average len of: {np.percentile([len(word) for word in hindi_sen],per)}\")\n",
    "print(f\"{per} percent of english sentence has average len of: {np.percentile([len(word) for word in english_sen],per)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "\n",
    "def valid_len_sentence(sentence,max_seq_len):\n",
    "    return len(list(sentence)) < (max_seq_len-1)\n",
    "\n",
    "def valid_sentence_token(sentence,vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "valid_index = []\n",
    "\n",
    "for index in range(len(hindi_sen)):\n",
    "    hindi_sentence, english_sentence = hindi_sen[index], english_sen[index]\n",
    "    if valid_len_sentence(hindi_sentence,max_seq_len) and valid_len_sentence(english_sentence,max_seq_len) \\\n",
    "    and valid_sentence_token(hindi_sentence,hindi_vocab):\n",
    "        valid_index.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8913)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_sen), len(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9074"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_index[8090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentence = [hindi_sen[i] for i in valid_index]\n",
    "english_sentence =  [english_sen[i] for i in valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8913, 8913)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_sentence), len(english_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 198)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(sen) for sen in hindi_sentence),max(len(sen) for sen in english_sentence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentence , hindi_sentence):\n",
    "        self.english_sentence = english_sentence\n",
    "        self.hindi_sentence = hindi_sentence\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentence)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        return self.english_sentence[idx] , self.hindi_sentence[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataSet(english_sentence,hindi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8913"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.\\n',\n",
       " 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 3\n",
    "train_data_loader = DataLoader(dataset,batchSize)\n",
    "iterator = iter(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\\n\", 'Whosoever desires the reward of the world, with Allah is the reward of the world and of the Everlasting Life. Allah is the Hearer, the Seer.\\n', 'The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness.\\n'), ('आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n', 'और जो शख्स (अपने आमाल का) बदला दुनिया ही में चाहता है तो ख़ुदा के पास दुनिया व आख़िरत दोनों का अज्र मौजूद है और ख़ुदा तो हर शख्स की सुनता और सबको देखता है\\n', 'जैव-मंडल में कीड़ों का मूल्य बहुत है, क्योंकि प्रजातियों की समृद्धि के मामले में उनकी संख्या अन्य जीव समूहों से ज़्यादा है।\\n')]\n",
      "[('Mithali To Anchor Indian Team Against Australia in ODIs\\n', 'After the assent of the Honble President on 8thSeptember, 2016, the 101thConstitutional Amendment Act, 2016 came into existence\\n', 'The court has fixed a hearing for February 12\\n'), ('आस्ट्रेलिया के खिलाफ वनडे टीम की कमान मिताली को\\n', '8 सितम्\\u200dबर, 2016 को माननीय राष्\\u200dट्रपति की स्\\u200dवीकृति मिलने के बाद 101वां संविधान संशोधन अधिनियम, 2016 अस्तित्\\u200dव में आया\\n', 'अदालत ने इस मामले में आगे की सुनवाई के लिए एक फरवरी की तारीख़ तय की\\n')]\n",
      "[('Please select the position where the track should be split.\\n', 'As per police, armys 22RR, special operation Group (SOG) of police and the Central Reserve Police Force (CRPF) cordoned the village and launched search operation in the area.\\n', 'Jharkhand chief minister Hemant Soren\\n'), ('जहाँ पर ट्रैक को विभाजित किया जाना है, कृपया वह स्थान चुनें.\\n', 'इसके तुरंत बाद सेना की 22 राष्ट्रीय राइफल्स (आरआर), सीआरपीएफ और पुलिस के स्पेशल ऑपरेशन ग्रुप (एसओजी) के जवानों द्वारा इलाके की घेराबंदी कर तलाशी अभियान चलाया।\\n', 'झारखंड के मुख्यमंत्री हेमंत सोरेन (फोटोः पीटीआई)\\n')]\n",
      "[('Arvind Kumar, SHO of the sector 55/56 police station, said a case has been registered under section 376-D (gang rape) of the Indian Penal Code.\\n', \"Briefing media in New Delhi today, party's State-in-charge, Anil Jain said, after the meeting the party will meet the Governor to stake claim for government formation in the state.\\n\", '\"Jesus responded, as he taught in the temple, \"\"How is it that the scribes say that the Christ is the son of David?\"\\n'), ('सेक्टर 55/56 के एसएचओ अरविंद कुमार ने बताया कि इस मामले में आईपीसी की धारा 376-डी (गैंगरेप) के तहत मामला दर्ज कर लिया गया है।\\n', 'आज नई दिल्ली में मीडिया से बातचीत में पार्टी के राज्य प्रभारी अनिल जैन ने बताया कि बैठक के बाद पार्टी, सरकार के गठन का दावा पेश करने के लिए राज्यपाल से मिलेगी।\\n', 'फिर यीशु ने मन्दिर में उपदेश करते हुए यह कहा, कि शास्त्री क्योंकर कहते हैं, कि मसीह दाऊद का पुत्रा है?\\n')]\n",
      "[('Senior leaders of all major parties held electioneering in favour of their candidates.\\n', 'Meanwhile, three people came there on a bike.\\n', 'Katrina shared the video on Instagram.\\n'), ('सभी मुख्य पार्टियों के वरिष्ठ नेताओं ने अपने-अपने उम्मीदवारों के पक्ष में चुनाव प्रचार किया।\\n', 'इसी बीच एक बाइक पर तीन लोग आते दिखाई दिए।\\n', 'कटरीना ने ये वीड\\u200dियो अपनी इंस्\\u200dटास्\\u200dटोरी में शेयर क\\u200dिया है.\\n')]\n"
     ]
    }
   ],
   "source": [
    "for i,sen in enumerate(iterator):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentence,lang_to_index,start_token=True,end_token=True):\n",
    "    token = encoder(sentence,lang_to_index)\n",
    "    if start_token:\n",
    "        token.insert(0,lang_to_index[START_TOKEN])\n",
    "    if end_token:\n",
    "        token.append(lang_to_index[END_TOKEN])\n",
    "    for _ in range(len(token),max_seq_len):\n",
    "        token.append(lang_to_index[PADDING_TOKEN])\n",
    "    return torch.tensor(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenization(hindi_sentence[3],hindi_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, tensor([  0, 128, 173, 190, 150, 190, 167, 184, 169, 177]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token),token[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8913,\n",
       " \"However, Paes, who was partnering Australia's Paul Hanley, could only go as far as the quarterfinals where they lost to Bhupathi and Knowles\\n\")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0:][0]), dataset[0][0] # all enghish sentences in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8913,\n",
       " 'आस्ट्रेलिया के पाल हेनली के साथ जोड़ी बनाने वाले पेस मियामी में क्वार्टरफाइनल तक ही पहुंच सके क्योंकि इस दौर में उन्हें भूपति और नोल्स ने हराया था।\\n')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[:][1]),dataset[0][1] # all hindi sentences in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_token,hin_token = [],[]\n",
    "for idx,batch_sentence in enumerate(train_data_loader):\n",
    "    if idx == 5:\n",
    "        break\n",
    "    for sentence_num in range(len(batch_sentence[0])):\n",
    "        eng , hindi = batch_sentence[0][sentence_num] , batch_sentence[1][sentence_num]\n",
    "        eng_token.append(tokenization(eng,english_to_index,start_token=False,end_token=False))\n",
    "        hin_token.append(tokenization(hindi,hindi_to_index))\n",
    "    \n",
    "   \n",
    "eng_token = torch.stack((eng_token))\n",
    "hin_token = torch.stack(hin_token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " tensor([[41, 79, 87,  ..., 94, 94, 94],\n",
       "         [56, 72, 79,  ..., 94, 94, 94],\n",
       "         [53, 72, 69,  ..., 94, 94, 94],\n",
       "         ...,\n",
       "         [52, 69, 78,  ..., 94, 94, 94],\n",
       "         [46, 69, 65,  ..., 94, 94, 94],\n",
       "         [44, 65, 84,  ..., 94, 94, 94]]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_token[0]),eng_token[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " tensor([[  0, 128, 173,  ..., 232, 232, 232],\n",
       "         [  0, 139, 167,  ..., 232, 232, 232],\n",
       "         [  0, 147, 185,  ..., 232, 232, 232],\n",
       "         ...,\n",
       "         [  0, 173, 164,  ..., 232, 232, 232],\n",
       "         [  0, 129, 173,  ..., 232, 232, 232],\n",
       "         [  0, 140, 150,  ..., 232, 232, 232]]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hin_token[0]),hin_token[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "ahed = torch.full([10,10],True)\n",
    "print(ahed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ahed = torch.triu(ahed,diagonal=1)\n",
    "ahed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "look = torch.full([4,4],True)\n",
    "look = torch.triu(look,diagonal=1)\n",
    "\n",
    "dec = torch.tensor([[[True,False,False,True],\n",
    "                    [False,False,False,True],\n",
    "                    [False,False,False,True],\n",
    "                    [False,False,False,True]],\n",
    "                    [[False, False, False,  True],\n",
    "        [False, False, False,  True],\n",
    "        [False, False, False,  True],\n",
    "        [False, False, False,  True]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ True, False, False,  True],\n",
       "          [False, False, False,  True],\n",
       "          [False, False, False,  True],\n",
       "          [False, False, False,  True]],\n",
       " \n",
       "         [[False, False, False,  True],\n",
       "          [False, False, False,  True],\n",
       "          [False, False, False,  True],\n",
       "          [False, False, False,  True]]]),\n",
       " tensor([[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec,look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]],\n",
       "\n",
       "        [[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = look+dec\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskCreation(english_batch,hindi_batch):\n",
    "    NEG_INFTY = -1e9\n",
    "    max_len = 200\n",
    "    number_sen = len(english_batch)\n",
    "    english_sen_len , hindi_sen_len = english_batch[0] , hindi_batch[0]\n",
    "    look_ahed_mask = torch.full([max_len,max_len],True)\n",
    "    look_ahed_mask = torch.triu(look_ahed_mask,diagonal=1)\n",
    "    encoder_padding_mask = torch.full([number_sen,max_len,max_len],False)\n",
    "    decoder_self_padding_mask = torch.full([number_sen,max_len,max_len],False)\n",
    "    decoder_cross_padding_mask = torch.full([number_sen,max_len,max_len],False)\n",
    "\n",
    "    for idx in range(sentence_num):\n",
    "        english_sen_len , hindi_sen_len = len(english_batch[idx]) , len(hindi_batch[idx])\n",
    "        english_char_padding = np.arange(english_sen_len+1,max_len)\n",
    "        hindi_char_padding = np.arange(hindi_sen_len+1,max_len)\n",
    "        encoder_padding_mask[idx,:,english_char_padding]=True\n",
    "        encoder_padding_mask[idx,english_char_padding,:] = True\n",
    "        decoder_self_padding_mask[idx,:,hindi_char_padding] = True\n",
    "        decoder_self_padding_mask[idx,hindi_char_padding,:] = True\n",
    "        decoder_cross_padding_mask[idx,:,english_char_padding] = True\n",
    "        decoder_cross_padding_mask[idx,hindi_char_padding,:] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask,NEG_INFTY,0)\n",
    "    decoder_self_attention_mask = torch.where(look_ahed_mask+decoder_self_padding_mask,NEG_INFTY,0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_cross_padding_mask,NEG_INFTY,0)\n",
    "    print(f\"size fo encoder mask = {encoder_self_attention_mask.size()}\\n{encoder_self_attention_mask[0,:1,:100]}\")\n",
    "    print(f\"size fo decoder self mask = {decoder_self_attention_mask.size()}\\n{decoder_self_attention_mask[0,:10,:15]}\")\n",
    "    print(f\"size fo decoder cross mask = {decoder_cross_attention_mask.size()}\\n{decoder_cross_attention_mask[0,:1,:100]}\")\n",
    "\n",
    "    return encoder_self_attention_mask ,decoder_self_attention_mask,decoder_cross_attention_mask\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 75)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sen[0][0]),len(sen[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size fo encoder mask = torch.Size([3, 200, 200])\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n",
      "size fo decoder self mask = torch.Size([3, 200, 200])\n",
      "tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n",
      "size fo decoder cross mask = torch.Size([3, 200, 200])\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "         -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n"
     ]
    }
   ],
   "source": [
    "mask = MaskCreation(sen[0],sen[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_embeddings(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_len):\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self):\n",
    "        even_idx = torch.arange(0,self.d_model,2)\n",
    "        # odd_idx = torch.arange(1,d_model,2)\n",
    "        denominator = torch.pow(10000,even_idx/self.d_model)\n",
    "        # print(denominator)\n",
    "        pos = torch.arange(self.max_seq_len,dtype=torch.float64).reshape(self.max_seq_len,1)\n",
    "        # print(pos)\n",
    "        even_pe = torch.sin(pos/denominator)\n",
    "        # print(even_pe)\n",
    "        odd_pe = torch.cos(pos/denominator)\n",
    "        # print(odd_pe)\n",
    "        stacked = torch.stack([even_pe,odd_pe],dim=2)\n",
    "        # print(stacked)\n",
    "        PE = torch.flatten(stacked,start_dim=1,end_dim=2)\n",
    "\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = Positional_embeddings(d_model=512,max_seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 7.9581e-01, -6.0555e-01,  9.9961e-01,  ...,  9.9978e-01,\n",
       "          2.0420e-02,  9.9979e-01],\n",
       "        [-7.9579e-02, -9.9683e-01,  5.9249e-01,  ...,  9.9977e-01,\n",
       "          2.0524e-02,  9.9979e-01],\n",
       "        [-8.8180e-01, -4.7163e-01, -3.2453e-01,  ...,  9.9977e-01,\n",
       "          2.0628e-02,  9.9979e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self,max_seq_len,d_model,language_to_index,PADDING_TOKEN,START_TOKEN,END_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.language_to_index = language_to_index\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.embeddings = nn.Embedding(self.vocab_size,self.d_model)\n",
    "        self.positon_encoding = Positional_embeddings(self.d_model,self.max_seq_len) # type: ignore\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def batch_token(self,batch,start_token=True,end_token=True):\n",
    "        def tokenizer(sentence,start_token=True,end_token=True):\n",
    "            sentence_to_token = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_to_token.insert(0,self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_to_token.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range((len(sentence_to_token)),self.max_seq_len):\n",
    "                sentence_to_token.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_to_token)\n",
    "        \n",
    "        tokens=[]\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            tokens.append(tokenizer(batch[i],start_token,end_token))\n",
    "        tokens = torch.stack(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def forward(self,btc,start_token = True):\n",
    "        x = self.batch_token(btc)\n",
    "        # print(x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        # print(x.shape)\n",
    "        pos = self.positon_encoding.forward()\n",
    "        # print(pos.shape)\n",
    "        x = self.dropout(x+pos)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emb_sen = SentenceEmbedding(max_seq_len=200,\n",
    "                            d_model=512,\n",
    "                            language_to_index=hindi_to_index,\n",
    "                            PADDING_TOKEN=PADDING_TOKEN,\n",
    "                            START_TOKEN=START_TOKEN,\n",
    "                            END_TOKEN=END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('वह ऐसा इसलिए करता है, क्योंकि वह भारतीय तिरंगे के कथित अनादर से क्रोधित है\\n',\n",
       " 'रंधावा से एक हफ्ते से उनके मोबाईल पर सम्पर्क करने की कोशिश की गयी, लेकिन बात नहीं हो पायी।\\n',\n",
       " 'नैनो की तुलना में अल्टो 800 का डिज़ाइन वेवफ्रंट है जो पारंपरिक कार की तरह दिखता है।\\n')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_obj = Emb_sen.forward(btc=sen[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8377,  2.2172,  1.2248,  ...,  2.9184, -0.8868,  2.0046],\n",
       "         [ 0.4725,  1.7880,  0.3720,  ...,  0.8701,  0.1268,  1.0720],\n",
       "         [ 1.7007,  1.0310,  0.4041,  ...,  1.5898, -0.0000,  0.6939],\n",
       "         ...,\n",
       "         [ 0.8994, -1.7061,  1.6693,  ...,  2.2401, -0.4364, -0.0000],\n",
       "         [-0.0732, -2.1409,  0.0000,  ...,  2.2401, -0.4362, -1.0666],\n",
       "         [-0.9646, -1.5573,  0.1980,  ...,  2.2401, -0.4361, -1.0666]],\n",
       "\n",
       "        [[ 0.8377,  2.2172,  1.2248,  ...,  2.9184, -0.8868,  0.0000],\n",
       "         [ 0.7482, -0.2052,  1.5270,  ..., -0.3135,  0.0000,  0.4252],\n",
       "         [ 0.7723, -0.1587, -0.9296,  ..., -0.3659, -1.8049, -0.5281],\n",
       "         ...,\n",
       "         [ 0.8994, -1.7061,  1.6693,  ...,  2.2401, -0.0000, -1.0666],\n",
       "         [-0.0732, -2.1409,  1.2169,  ...,  2.2401, -0.4362, -0.0000],\n",
       "         [-0.9646, -1.5573,  0.0000,  ...,  2.2401, -0.4361, -0.0000]],\n",
       "\n",
       "        [[ 0.8377,  2.2172,  1.2248,  ...,  2.9184, -0.8868,  0.0000],\n",
       "         [ 0.2484, -0.3307,  1.7488,  ...,  1.2401, -0.0000,  0.2719],\n",
       "         [ 0.1735, -1.2042,  1.8152,  ...,  2.0446,  1.2893,  0.9501],\n",
       "         ...,\n",
       "         [ 0.0000, -1.7061,  1.6693,  ...,  2.2401, -0.4364, -1.0666],\n",
       "         [-0.0732, -2.1409,  1.2169,  ...,  2.2401, -0.4362, -1.0666],\n",
       "         [-0.9646, -1.5573,  0.1980,  ...,  2.2401, -0.4361, -1.0666]]],\n",
       "       dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200, 512])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_obj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
